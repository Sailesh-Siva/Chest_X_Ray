import os
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay
import numpy as np
# Define directories for the data
train_dir = r'C:\Users\Sailesh\Files\Coding\Python\Projects\ChestXRay\Dataset\train'
test_dir = r'C:\Users\Sailesh\Files\Coding\Python\Projects\ChestXRay\Dataset\test'
val_dir = r'C:\Users\Sailesh\Files\Coding\Python\Projects\ChestXRay\Dataset\val'

# Function to display images with file names and class counts
def display_images_with_filenames(directory, num_images=10):
    images = []
    labels = []
    file_names = []
    class_counts = {}
    
    for class_name in os.listdir(directory):
        class_dir = os.path.join(directory, class_name)
        if os.path.isdir(class_dir):
            class_counts[class_name] = len(os.listdir(class_dir))
            
            # Limit images per class and avoid division errors
            images_per_class = max(1, num_images // len(os.listdir(directory)))
            for img_file in os.listdir(class_dir)[:images_per_class]:
                img_path = os.path.join(class_dir, img_file)
                images.append(img_path)
                labels.append(class_name)
                file_names.append(img_file)
    
    # Display images
    plt.figure(figsize=(15, 10))
    for i, img_path in enumerate(images[:num_images]):
        img = load_img(img_path, target_size=(150, 150))
        plt.subplot(2, 5, i + 1)
        plt.imshow(img)
        plt.title(f"{labels[i]}: {file_names[i]}", fontsize=8)
        plt.axis('off')
    
    plt.suptitle("Sample Images from Dataset", fontsize=16)
    plt.tight_layout()
    plt.show()

    # Print filenames and their classes
    print("\nImage Names and Classes:")
    for i in range(min(num_images, len(images))):
        print(f"Class: {labels[i]}, File Name: {file_names[i]}")

    # Print number of images per class
    print("\nNumber of Images per Class:")
    for class_name, count in class_counts.items():
        print(f"Class: {class_name}, Count: {count}")

# Display images
display_images_with_filenames(train_dir, num_images=10)
display_images_with_filenames(test_dir, num_images=10)
display_images_with_filenames(val_dir, num_images=10)

# Data augmentation and generators
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)
test_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary',
    shuffle=False  # Important for metrics calculation
)

val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary'
)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Input)
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import AdamW



# Build the CNN model

def create_improved_model(input_shape=(150, 150, 3)):
    model = Sequential()

    # Input Layer
    model.add(Input(shape=input_shape))

    # Convolutional Block 1
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001)))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.2))  # Dropout added here

    # Convolutional Block 2
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.3))  # Dropout added here

    # Convolutional Block 3
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.3))  # Dropout added here

    # Convolutional Block 4
    model.add(Conv2D(256, (3, 3), activation='relu', kernel_regularizer=l2(0.001)))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.4))  # Dropout added here

    # Global Average Pooling instead of Flatten
    model.add(GlobalAveragePooling2D())

    # Fully Connected Layers
    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))
    model.add(Dropout(0.5))  # Dropout for fully connected layer
    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))
    model.add(Dropout(0.5))  # Dropout for fully connected layer
    model.add(Dense(1, activation='sigmoid'))  # Binary classification

    optimizer = AdamW(learning_rate=0.001, weight_decay=1e-4)
    model.compile(optimizer=optimizer,
              loss='binary_crossentropy',
              metrics=['accuracy'])
    # Compile the model

    return model


# Create the improved model
model = create_improved_model()

# Summary of the model
model.summary()



from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

# Callback 1: Save model weights at every epoch
checkpoint_callback = ModelCheckpoint(
    filepath='weights_epoch_{epoch:02d}.weights.h5',  # Filepath with epoch number
    save_weights_only=True,                          # Save only weights, not the full model
    save_freq='epoch',                               # Save after every epoch
    verbose=1                                        # Print a message after saving
)

# Callback 3: Reduce learning rate when a plateau in validation loss is observed
lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss',              # Monitor validation loss
    factor=0.5,                      # Reduce learning rate by 50%
    patience=3,                      # Wait for 3 epochs before reducing LR
    min_lr=1e-6,                     # Minimum learning rate
    verbose=1
)

# Compile the model (recompile to ensure all changes are applied)
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Train the model

history = model.fit(
    train_generator,                # Training data generator
    epochs=40,                      # Number of epochs
    val_data=val_generator,  # Validation data generator
    callbacks=[checkpoint_callback, lr_scheduler]  # Add callbacks
)

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

# Loop through models from epoch 1 to 40
for epoch in range(1, 41):
    weights_file = f"weights_epoch_{epoch:02d}.weights.h5"
    print(f"\nEvaluating Model for Epoch {epoch}:")
    
    try:
        # Load weights for the current epoch
        model.load_weights(weights_file)
        
        # Get true labels and predicted labels for the test set
        test_generator.reset()  # Reset the generator to iterate through the data
        y_true = test_generator.classes  # True labels
        y_pred_probs = model.predict(test_generator)  # Predicted probabilities
        y_pred = (y_pred_probs > 0.5).astype(int).flatten()  # Convert probabilities to binary labels

        # Compute the confusion matrix
        conf_matrix = confusion_matrix(y_true, y_pred)

        # Display the confusion matrix
        disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=test_generator.class_indices.keys())
        disp.plot(cmap='Blues')
        plt.title(f"Confusion Matrix for Epoch {epoch}")
        plt.show()

        # Print the classification report
        print("Classification Report:")
        print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))
    
    except FileNotFoundError:
        print(f"Warning: Weights file for epoch {epoch} not found.")
        continue

import matplotlib.pyplot as plt

# Assuming 'history' is the History object returned by model.fit()
def plot_training_curves(history):
    # Loss plot
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.show()

    # Accuracy plot
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()
    plt.show()

plot_training_curves(history)
